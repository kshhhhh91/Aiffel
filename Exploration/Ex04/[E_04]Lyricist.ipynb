{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76bd0de1",
   "metadata": {},
   "source": [
    "# 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db8242e",
   "metadata": {},
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0ead102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 불러오기\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee2f6e2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\", 'It goes like this', 'The fourth, the fifth']\n"
     ]
    }
   ],
   "source": [
    "# 데이터 읽어오기\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담기\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc0a869",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3b8fa2",
   "metadata": {},
   "source": [
    "### 데이터 정제하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17080ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료\n"
     ]
    }
   ],
   "source": [
    "# 문장 정제 함수 만들기\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 소문자로 바꾸고, 양쪽 공백 지우기\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 특수문자 양쪽에 공백 넣기\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) #  여러개의 공백을 하나의 공백으로 바꾸기\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꾸기\n",
    "    sentence = sentence.strip() # 위의 것들 바꾸면서 다시 생긴 양쪽 공백 지우기\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 문장 시작에 <start>, 끝에 <end> 추가해주기\n",
    "    return sentence\n",
    "\n",
    "print(\"완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b9b9aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start> now i ve heard there was a secret chord <end>', '<start> that david played , and it pleased the lord <end>', '<start> but you don t really care for music , do you ? <end>', '<start> it goes like this <end>', '<start> the fourth , the fifth <end>', '<start> the minor fall , the major lift <end>', '<start> the baffled king composing hallelujah hallelujah <end>', '<start> hallelujah <end>', '<start> hallelujah <end>', '<start> hallelujah your faith was strong but you needed proof <end>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "156013"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정제된 문장 모으기\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    preprocessed_sentence = preprocess_sentence(sentence) # 문장 정제 먼저 하기\n",
    "    if len(sentence) == 0: continue # 빈 문장이면 제외하기\n",
    "    if sentence[-1] == \":\": continue # \":\"으로 끝나면 제외하기\n",
    "    if len(preprocessed_sentence.split()) > 15: continue # 토큰 길이 15 넘어가면 제외하기\n",
    "    corpus.append(preprocessed_sentence) # 문장 모아주기\n",
    "    \n",
    "# 정제된 결과 확인\n",
    "print(corpus[:10])\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "231ccd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    4 ...    0    0    0]\n",
      " [   2   15 2967 ...    0    0    0]\n",
      " [   2   33    7 ...   46    3    0]\n",
      " ...\n",
      " [   2    4  118 ...    0    0    0]\n",
      " [   2  258  194 ...   12    3    0]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f7bcefa88e0>\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer 함수 만들기\n",
    "\n",
    "def tokenize(corpus):\n",
    "    # 12,000 단어 이상의 단어를 기억할 수 있는 tokenizer 만들기\n",
    "    # 12,000 단어에 포함되지 못한 단어는 '<unk>'로 바꾸기\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=12000,\n",
    "                                                     filters=' ',\n",
    "                                                     oov_token=\"<unk>\")\n",
    "    # corpus 이용해서 tokenizer 내부의 단어장 완성\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰주기\n",
    "    # 시퀀스가 짧다면 문장 뒤에 패딩 붙여서 길이 맞춰주기 (padding='post')\n",
    "    # (만약 문장 앞에 패딩 붙여서 길이 맞추고 싶으면 padding='pre'를 사용하기)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    \n",
    "    print(tensor, tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "# tokenize() 함수로 데이터를 Tensor로 변환해주기\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1df21f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    4   95  303   62   53    9  946 6263    3    0    0    0\n",
      "     0]\n",
      " [   2   15 2967  871    5    8   11 5739    6  374    3    0    0    0\n",
      "     0]\n",
      " [   2   33    7   40   16  164  288   28  333    5   48    7   46    3\n",
      "     0]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3]) # 모두 정수로 이루어져 있음, 이 숫자들은 tokenizer에 구축된 단어 사전의 인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77435893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156013, 15)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape # (정제된 문장들의 총 개수 : 156013개, 토큰의 최대 개수 : 15개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9c7fe64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "# 단어사전이 어떻게 구축되어 있는지 확인해보기\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bb2002e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   50    4   95  303   62   53    9  946 6263    3    0    0    0]\n",
      "[  50    4   95  303   62   53    9  946 6263    3    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# 생성된 텐서를 소스와 타겟으로 분리하기\n",
    "\n",
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성\n",
    "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높음\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8922e0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3874fe",
   "metadata": {},
   "source": [
    "### 훈련, 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e617bb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124810, 14)\n",
      "Target Train: (124810, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input,\n",
    "                                                          tgt_input,\n",
    "                                                          test_size=0.2,\n",
    "                                                          random_state=42)\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a617145",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd18ead",
   "metadata": {},
   "source": [
    "### 훈련 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abca1ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(enc_train)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
    "\n",
    " # tokenizer가 구축한 단어사전 내 12000개와, 여기 포함되지 않은 0:<pad>를 포함하여 12001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋을 생성\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744cb86f",
   "metadata": {},
   "source": [
    "### TextGenerator 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c3e0dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "534c6e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[-9.55585419e-05, -1.56625771e-04, -1.56642636e-04, ...,\n",
       "          1.24469907e-05, -9.50043159e-06,  9.23849220e-05],\n",
       "        [-7.85886914e-06, -2.17994413e-04, -4.67605918e-04, ...,\n",
       "         -3.73620423e-05,  8.62866873e-05, -2.46967567e-04],\n",
       "        [ 1.10928675e-04, -1.67456776e-04, -8.38014123e-04, ...,\n",
       "          4.75334637e-05, -1.74995330e-05, -4.29671461e-04],\n",
       "        ...,\n",
       "        [ 4.08146996e-04,  1.53296941e-03,  1.15087285e-04, ...,\n",
       "          1.91686529e-04,  9.33270436e-04, -2.75532046e-04],\n",
       "        [ 4.33736684e-04,  2.04587914e-03,  2.80478504e-04, ...,\n",
       "          3.89598776e-04,  8.05802352e-04, -5.92254873e-05],\n",
       "        [ 4.24223166e-04,  2.51314719e-03,  3.74554191e-04, ...,\n",
       "          5.39924949e-04,  6.25654473e-04,  1.15092625e-04]],\n",
       "\n",
       "       [[-9.55585419e-05, -1.56625771e-04, -1.56642636e-04, ...,\n",
       "          1.24469907e-05, -9.50043159e-06,  9.23849220e-05],\n",
       "        [-1.30632223e-04, -5.90043128e-05, -3.83987150e-04, ...,\n",
       "          3.41699080e-04,  4.06134677e-06,  1.53214482e-04],\n",
       "        [-1.90880237e-04,  1.08064734e-04, -6.08213537e-04, ...,\n",
       "          6.86000276e-04, -1.21708297e-04,  1.52481196e-04],\n",
       "        ...,\n",
       "        [-8.99729930e-05,  2.11670366e-03,  6.52636285e-04, ...,\n",
       "          1.86635822e-03,  2.43145259e-04,  1.34068762e-03],\n",
       "        [-7.89067490e-05,  2.56352592e-03,  6.69007655e-04, ...,\n",
       "          1.83944881e-03,  9.55535361e-05,  1.27960939e-03],\n",
       "        [-7.14913913e-05,  2.95325159e-03,  6.27420493e-04, ...,\n",
       "          1.75823143e-03, -6.85459381e-05,  1.20446179e-03]],\n",
       "\n",
       "       [[-9.55585419e-05, -1.56625771e-04, -1.56642636e-04, ...,\n",
       "          1.24469907e-05, -9.50043159e-06,  9.23849220e-05],\n",
       "        [ 7.83347787e-06, -2.83990346e-04, -3.59138430e-05, ...,\n",
       "         -1.58531286e-04, -9.69508983e-05,  1.10058412e-04],\n",
       "        [ 3.34669458e-05, -2.03000862e-04,  1.91107916e-04, ...,\n",
       "         -3.74876909e-05,  3.08794988e-05,  4.59968869e-04],\n",
       "        ...,\n",
       "        [ 6.94230184e-05,  3.08175501e-03,  7.95995642e-04, ...,\n",
       "          1.13280397e-03,  1.16782459e-04,  1.00561837e-03],\n",
       "        [ 6.46210465e-05,  3.37913842e-03,  6.54493691e-04, ...,\n",
       "          1.09349447e-03, -6.80114390e-05,  9.71589296e-04],\n",
       "        [ 5.70176744e-05,  3.63256200e-03,  5.02790324e-04, ...,\n",
       "          1.02310826e-03, -2.55763502e-04,  9.29018541e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-9.55585419e-05, -1.56625771e-04, -1.56642636e-04, ...,\n",
       "          1.24469907e-05, -9.50043159e-06,  9.23849220e-05],\n",
       "        [-3.13127442e-04,  9.02117827e-05, -1.61605683e-04, ...,\n",
       "          1.99297283e-04, -1.47855564e-04, -4.68194667e-05],\n",
       "        [-2.47039745e-04,  2.39194633e-04, -1.74226516e-04, ...,\n",
       "          1.74124973e-04, -1.39933269e-04, -1.23429618e-05],\n",
       "        ...,\n",
       "        [-8.67557319e-05,  3.59903648e-03,  4.02293168e-04, ...,\n",
       "          7.99666974e-04, -3.51796480e-04,  7.16926821e-04],\n",
       "        [-9.05990601e-05,  3.83900548e-03,  3.00165324e-04, ...,\n",
       "          7.57632894e-04, -5.37792803e-04,  7.00311677e-04],\n",
       "        [-8.98764702e-05,  4.03324096e-03,  1.90715349e-04, ...,\n",
       "          6.99399505e-04, -7.15747534e-04,  6.78935088e-04]],\n",
       "\n",
       "       [[-9.55585419e-05, -1.56625771e-04, -1.56642636e-04, ...,\n",
       "          1.24469907e-05, -9.50043159e-06,  9.23849220e-05],\n",
       "        [-2.40303503e-04, -2.04524564e-04, -3.57182231e-04, ...,\n",
       "         -1.12379712e-04, -1.28338681e-04,  2.00202470e-04],\n",
       "        [-1.65922553e-04, -1.92917592e-04, -7.27141276e-04, ...,\n",
       "         -2.50197161e-04, -1.00039404e-04, -2.01264309e-04],\n",
       "        ...,\n",
       "        [-2.06931014e-04,  2.51895655e-03, -3.57299839e-04, ...,\n",
       "          1.06243580e-03, -1.22144833e-04,  1.54547539e-04],\n",
       "        [-2.42128765e-04,  2.90254713e-03, -2.06437704e-04, ...,\n",
       "          1.08319218e-03, -2.51918274e-04,  3.01926397e-04],\n",
       "        [-2.72541161e-04,  3.23895458e-03, -1.22109472e-04, ...,\n",
       "          1.07400294e-03, -3.91689216e-04,  4.06727369e-04]],\n",
       "\n",
       "       [[-9.55585419e-05, -1.56625771e-04, -1.56642636e-04, ...,\n",
       "          1.24469907e-05, -9.50043159e-06,  9.23849220e-05],\n",
       "        [-4.92915460e-05, -2.49715784e-04, -2.89300715e-06, ...,\n",
       "          1.05168991e-04,  1.60212352e-04,  1.21766774e-04],\n",
       "        [-5.28355340e-05, -1.57619492e-04,  2.96054459e-05, ...,\n",
       "          2.81495159e-04,  3.65642365e-04,  1.58893599e-04],\n",
       "        ...,\n",
       "        [-6.58073172e-04,  1.82489341e-03,  1.18426408e-03, ...,\n",
       "          2.95248756e-04,  1.22787070e-03,  8.22758593e-04],\n",
       "        [-6.42994011e-04,  2.27952888e-03,  1.09705993e-03, ...,\n",
       "          4.22881276e-04,  9.38337937e-04,  8.15360050e-04],\n",
       "        [-6.20510487e-04,  2.68116826e-03,  9.63989994e-04, ...,\n",
       "          4.99630463e-04,  6.44463755e-04,  7.92754116e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한 배치만 불러온 데이터를 모델에 넣어보기\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e2fa24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59b3168",
   "metadata": {},
   "source": [
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6125cd5",
   "metadata": {},
   "source": [
    "### 모델 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83c7e88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "487/487 [==============================] - 78s 150ms/step - loss: 3.4941\n",
      "Epoch 2/10\n",
      "487/487 [==============================] - 77s 158ms/step - loss: 3.0142\n",
      "Epoch 3/10\n",
      "487/487 [==============================] - 79s 163ms/step - loss: 2.8520\n",
      "Epoch 4/10\n",
      "487/487 [==============================] - 81s 165ms/step - loss: 2.7303\n",
      "Epoch 5/10\n",
      "487/487 [==============================] - 82s 167ms/step - loss: 2.6260\n",
      "Epoch 6/10\n",
      "487/487 [==============================] - 82s 167ms/step - loss: 2.5323\n",
      "Epoch 7/10\n",
      "487/487 [==============================] - 82s 168ms/step - loss: 2.4461\n",
      "Epoch 8/10\n",
      "487/487 [==============================] - 82s 168ms/step - loss: 2.3664\n",
      "Epoch 9/10\n",
      "487/487 [==============================] - 82s 168ms/step - loss: 2.2917\n",
      "Epoch 10/10\n",
      "487/487 [==============================] - 82s 168ms/step - loss: 2.2217\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7aea28b1c0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cf2441",
   "metadata": {},
   "source": [
    "### 평가 데이터셋 생성 및 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "846d606b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(enc_val)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(enc_val) // BATCH_SIZE\n",
    "\n",
    " # tokenizer가 구축한 단어사전 내 12000개와, 여기 포함되지 않은 0:<pad>를 포함하여 12001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋을 생성\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val))\n",
    "test_dataset = test_dataset.shuffle(BUFFER_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9bfb37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121/121 [==============================] - 9s 67ms/step - loss: 2.5268\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가하기\n",
    "\n",
    "val_loss = model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c073f29",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a1955b",
   "metadata": {},
   "source": [
    "### 작사가 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48f3aaae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료\n"
     ]
    }
   ],
   "source": [
    "# generate_text 함수는 모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행하게 함.\n",
    "\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=15):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만들기\n",
    "    while True:\n",
    "        # 1. 입력받은 문장의 텐서를 입력\n",
    "        predict = model(test_tensor) \n",
    "        # 2. 예측된 값 중 가장 높은 확률인 word index를 뽑아내기\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3. 2에서 예측된 word index를 문장 뒤에 붙이기\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마침\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated\n",
    "\n",
    "print(\"완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8929f960",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cda88a",
   "metadata": {},
   "source": [
    "### 문장 생성해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c646baea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , liberian girl <end> '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03051c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i m gonna be a little selfish <end> '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b209ff0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you re the only one that s a star <end> '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10c14f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> long as i got a few ass niggas <end> '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47f4e970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i can t help it <end> '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i can\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f5086e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i miss you , i m bad <end> '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i miss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6cede2b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> it s not a statement i m not <end> '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba3745a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> good bye , i m a <unk> <end> '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18689f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> have you ever seen death singing <end> '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> have\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f6c610",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd7317f",
   "metadata": {},
   "source": [
    "### 하이퍼파라미터 바꿔서 다시 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64b3f1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 512\n",
    "hidden_size = 2048\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a344652d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "487/487 [==============================] - 244s 462ms/step - loss: 3.3631\n",
      "Epoch 2/10\n",
      "487/487 [==============================] - 227s 466ms/step - loss: 2.7992\n",
      "Epoch 3/10\n",
      "487/487 [==============================] - 227s 467ms/step - loss: 2.5182\n",
      "Epoch 4/10\n",
      "487/487 [==============================] - 227s 466ms/step - loss: 2.2432\n",
      "Epoch 5/10\n",
      "487/487 [==============================] - 227s 467ms/step - loss: 1.9801\n",
      "Epoch 6/10\n",
      "487/487 [==============================] - 227s 467ms/step - loss: 1.7353\n",
      "Epoch 7/10\n",
      "487/487 [==============================] - 227s 466ms/step - loss: 1.5219\n",
      "Epoch 8/10\n",
      "487/487 [==============================] - 227s 467ms/step - loss: 1.3434\n",
      "Epoch 9/10\n",
      "487/487 [==============================] - 227s 467ms/step - loss: 1.2025\n",
      "Epoch 10/10\n",
      "487/487 [==============================] - 227s 466ms/step - loss: 1.1009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7acce987c0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 다시 학습하기\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3ef5e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121/121 [==============================] - 21s 170ms/step - loss: 2.1834\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가하기\n",
    "\n",
    "val_loss = model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "557393d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , liberian girl , <end> '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8758698e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i m gonna be the mane event <end> '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "78389425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you know i m never gonna leave you <end> '"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b8882b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> long as i got stuff from most of em <end> '"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39ace36a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i miss you , i miss you <end> '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i miss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ba79c427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> it s a beautiful kind of pain <end> '"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b86a5040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> good day sunshine . can t buy me love , love <end> '"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c795f625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> have you seen death singing <end> '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> have\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e6889f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3674a4",
   "metadata": {},
   "source": [
    "### 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3552f3",
   "metadata": {},
   "source": [
    "val_loss가 처음에 2.5268이 나와서, 이것을 줄이기 위해서 하이퍼파라미터를 바꿔보았다.\n",
    "- embedding_size = 256 -> 512\n",
    "- hidden_size = 1024 -> 2048\n",
    "\n",
    "하이퍼파라미터를 바꾸고 나서 확인해보니 val_loss가 2.1834로 줄었다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13962dc",
   "metadata": {},
   "source": [
    "조금 의아한건 하이퍼파라미터를 바꾸기 전의 노랫말들이 조금 더 자연스러운듯한 느낌이 든다. 물론 어법이 맞지 않고 완성이 안된 문장들도 있긴하다. 문장도 조금 짧은 느낌이 들긴 하다.\n",
    "\n",
    "하이퍼파라미터를 바꾸고 난 뒤의 노랫말들은 조금 더 길어지긴 했는데, 단어 쓰임새가 틀린 경우도 있고, 약간 노랫말로 쓰기엔 어색한 느낌의 문장도 나온다.\n",
    "\n",
    "\n",
    "확실히 자연어 처리는 조금 어렵다. 데이터 정제하는 것을 이해하는데도 조금 시간이 걸리고, RNN과 관련된 모델들도 아직 익숙하지 않아서 더 발전시키기가 어려운 것 같다.\n",
    "나중에 공부를 좀더 해서 모델도 바꿔보고 하이퍼파라미터도 바꿔서 더 좋은 결과를 낼 수 있도록 해봐야겠다.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
